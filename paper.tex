% \documentclass[11pt]{article}
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

 
% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks 
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{tocloft}            % TOC spacing

%\usepackage{txfont}

\usepackage{enumitem}

\usepackage{breakcites}

\usepackage{etoolbox}
\usepackage{comment}
\newtoggle{draft}
\togglefalse{draft}
\newcommand{\draft}[1]{\iftoggle{draft}{#1}{}}


% \mathscr
\usepackage{mathrsfs}

\usepackage{algorithm}
\usepackage{verbatim}
\usepackage[noend]{algpseudocode}
\newcommand{\multiline}[1]{\parbox[t]{\dimexpr\linewidth-\algorithmicindent}{#1}}

\usepackage{multicol}

\usepackage{colortbl}

\usepackage{setspace}

\usepackage{transparent}

\usepackage{inconsolata}
\usepackage[scaled=.90]{helvet}
%\usepackage{fontspec}
% \usepackage{helvet}
\usepackage{xspace}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{bm}
\newcommand{\x}{\bm{x}}

% \usepackage{eufrak}
% \usepackage{fontspec,unicode-math}
% \setmathfont[range=\mathfrak]{Old English Text MT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Better looking lowercase mathfrak
% https://tex.stackexchange.com/questions/398710/alternative-separate-font-to-mathfrak-for-small-letters

\DeclareFontFamily{U}{jkpmia}{}
\DeclareFontShape{U}{jkpmia}{m}{it}{<->s*jkpmia}{}
\DeclareFontShape{U}{jkpmia}{bx}{it}{<->s*jkpbmia}{}
\DeclareMathAlphabet{\mathfrak}{U}{jkpmia}{m}{it}
\SetMathAlphabet{\mathfrak}{bold}{U}{jkpmia}{bx}{it}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{arxiv_style}
\input{dylan}
\input{macros}
\let\underbar\undefined
\input{widebar}

% \let\openbox\relax
% \usepackage{pxfonts}
% \let\openbox\relax
% \usepackage{txfonts}


\usepackage{color-edits}
 % \usepackage[suppress]{color-edits}
 \addauthor{df}{ForestGreen}
 \newcommand{\dfc}[1]{\dfcomment{#1}}

 % \usepackage{showlabels}

 % \usepackage[cut]{thmbox}

% \input{theorembox}
 


  % Indent option for \Statex
\makeatletter
\let\OldStatex\Statex
\renewcommand{\Statex}[1][3]{%
  \setlength\@tempdima{\algorithmicindent}%
  \OldStatex\hskip\dimexpr#1\@tempdima\relax}
\makeatother

% \let\vec\undefined
 \usepackage{accents}
 \newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\let\oldparagraph\paragraph
\newcommand{\paragraphi}[1]{\oldparagraph{\emph{#1}.}}
\renewcommand{\paragraph}[1]{\oldparagraph{#1.}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \newcommand{\Unif}{\mathsf{Unif}}

\newcommand{\phistar}{\phi^{\star}}
\newcommand{\gstar}{g^{\star}}

\newcommand{\meet}{\wedge} 
\newcommand{\nn}{\nonumber}

\renewcommand{\dagger}{\texttt{DAGGER}\xspace}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\title{}
\title{Self-Improvement and Bootstrapping}
\author{%
%
}


\date{}

\begin{document}
\maketitle

\tableofcontents

\section{Basic self-improvement setup}
Consider the following setup. The true model is $\pistar:\cX\to\Delta(\cY)$. We
initially receive $n$ samples $\cD=\crl*{(x,y)}$ where $x\sim\rho$ and
$y\sim\pistar$, and we fit $\pi\ind{1}
=\argmax_{\pi\in\Pi}\Eh\brk*{\log\pi(y\mid{}x)}$. We can consider
various self-training protocols
\begin{itemize}
\item \textbf{Regularized maximum likelihood}. We repeatedly do the following:
  \begin{itemize}
  \item Generate a dataset $\cD\ind{t}=\crl*{(x,y)}$ of samples with
    $x\sim\rho$ and $y\sim\pi\ind{t}$.
  \item Define
    \begin{align}
      \label{eq:regularized_mle}
      \pi\ind{t+1}=\argmax\crl*{
      \Eh_{\cD\ind{t}}\brk*{\log\pi(y\mid{}x)}
      -\beta\Dkl{\pi}{\pi\ind{t}}
      }.
    \end{align}
  \end{itemize}
  From first-order conditions, the maximizer here should satisfy
  \[
    \frac{\pi\ind{t}(y\mid{}x)}{\pi(y\mid{}x)}
    =     \beta\log\frac{\pi(y\mid{}x)}{\pi\ind{t}(y\mid{}x)}  + \lambda,
  \]
  where $\lambda\in\bbR$ is a Lagrange multiplier.

A version that has nicer first-order conditions (not sure if there's a
closed form yet) is
    \begin{align}
      \label{eq:regularized_mle_chi}
      \pi\ind{t+1}=\argmax\crl*{
      \Eh_{\cD\ind{t}}\brk*{\log\pi(y\mid{}x)}
      -\frac{\beta}{2}\Dchis{\pi}{\pi\ind{t}}
      }.
    \end{align}
This gives $\pi\ind{t}(y\mid{}x)^2 = \beta\pi^2(y\mid{}x) +
2\lambda\pi(y\mid{}x)\pi\ind{t}(y\mid{}x)$ for a Lagrange multiplier
$\lambda\in\bbR$. Solving the quadratic equation then gives
$\pi = \pi\ind{t}\cdot(\pm\sqrt{\lambda^2+\beta^2}-\lambda)/\beta$,
and setting $\lambda$ so that this normalizes to $1$ just gives $\pi\ind{t+1}=\pi\ind{t}$.
\item \textbf{Reverse maximum likelihood (Akshay's objective)}.
  We repeatedly do the following:
  \begin{itemize}
  \item Generate a dataset $\cD\ind{t}=\crl*{(x,y)}$ of samples with
    $x\sim\rho$ and $y\sim\pi\ind{t}$.
  \item Define
    \begin{align}
      \label{eq:self_training}
      \pi\ind{t+1}=\argmax\crl*{
      \En_{\pi}\brk*{\log\pi\ind{t}(y\mid{}x)}
      -\beta\Dkl{\pi}{\pi\ind{t}}
      }.
    \end{align}
  \end{itemize}
  With no errors, this converges to
  \[
    \pi\ind{t+1}(y\mid{}x)\propto{}\pistar(y\mid{}x)^{(1+\beta^{-1})^{T}}.
  \]
Here's an observation: If we define the entropy as
$H(\pi)=-\En_{x\sim\rho{},y\sim\pi}\brk*{\log\pi(y\mid{}x)}$, when
$\grad_{\pi}H=-\log(\pi(\cdot\mid{}\cdot)) + \mb{1}$. Hence, we can
interpret \cref{eq:self_training} as doing exponentiated gradient
\emph{ascent} on the convex objective
\[
R(\pi) = -H(\pi).
\]
That is, this is an instance of non-concave maximization, but the
local optimum is the argmax policy $\argmax_{y}\pistar(y\mid{}x)$.
  
  \item \textbf{Regularized maximum likelihood with reverse-KL regularization}. We repeatedly do the following:
  \begin{itemize}
  \item Generate a dataset $\cD\ind{t}=\crl*{(x,y)}$ of samples with
    $x\sim\rho$ and $y\sim\pi\ind{t}$.
  \item Define
    \begin{align}
      \label{eq:regularized_mle_reverse}
      \pi\ind{t+1}=\argmax\crl*{
      \Eh_{\cD\ind{t}}\brk*{\log\pi(y\mid{}x)}
      -\beta\Dkl{\pi\ind{t}}{\pi}
      }.
    \end{align}
  \end{itemize}
  \dfc{This is equivalent to just
    $\argmax_{\pi}(1+\beta)\En_{\pi\ind{t}}\brk*{\log\pi(y\mid{}x)}$,
    which means $\beta$ has no effect. Probably not the best choice}
\end{itemize}
\cref{eq:regularized_mle} is kind of like iterative SFT with
pseudo-labels, whereas \cref{eq:self_training} is closer to iterative
RLHF with pseudo-labels.
\subsection{Questions and comments}
We know that with no statistical or optimization errors, the estimator
in \cref{eq:self_training} should converge to
  \[
    \pi\ind{t+1}(y\mid{}x)\propto{}\pistar(y\mid{}x)^{(1+\beta^{-1})^{T}}
  \]
  % \dfc{Check this!}
  Questions:
\begin{itemize}
\item When can we achieve similar convergence in the presence of
  statistical errors? E.g., what are the tradeoffs for choosing $\beta$?
\item What are the different tradeoffs for \cref{eq:regularized_mle,eq:self_training,eq:regularized_mle_reverse}?
\item Is this purely a computational phenomenon?
  \begin{itemize}
  \item Concretely: Is $\pi\ind{t+1}$ ever better statistically than
    computing the sequence-level argmax
    $\argmax_{y}\pi\ind{1}(y\mid{}x)$? If not, then we want to argue
    that the benefit is that we are competing with the sequence level
    argmax without having to actually compute it (that is, given a new
    $x$, we just need to sample $y\sim{}\pi\ind{t+1}(y\mid{}x)$.
    \begin{itemize}
    \item This would be a sort of ``optimization vs sampling'' phenomenon.
    \end{itemize}
  \item Alternatively, we might be able to argue that our algo *does*
    have statistical benefits, because it allows us to leverage the
    unlabeled examples $x$s.
  \end{itemize}
\item Presumably we need some kind of margin condition to ensure that
  \cref{eq:self_training} actually converges to argmax instead of
  noise. This is most easy to think about when $\pistar$ is itself a
  softmax policy.
\item Suppose $\pi\ind{1}$ is an LLM with temperature parameter
  $\eta$. We have $y=y_{1:H}$, and
  $\pi(y_h\mid{}x,y_{1:h-1})\propto\exp\prn*{\frac{f(x,y_{1:h})}{\eta}}$. A
  good baseline for us is to set $\eta\to{}0$ after training. In this
  case, sampling autoregressively is the same as choosing the greedy
  argmax at each step $h$,
  i.e. $y_h=\argmax{}\pi(y_h\mid{}x,y_{1:h-1})$, but is not
  necessarily equivalent to the sequence-level argmax.
  \begin{itemize}
  \item Hence, we might hope that self-training can improve over this
    baseline, even if it can't improve over the true sequence-level
    argmax.
  \item Good example to think about above: $f=\Qstarb$ is the soft
    Q-function for a given reward function $r$.
  \end{itemize}
\item In reality, we will not be able to evaluate the KL term in
  \cref{eq:self_training} exactly, due to statistical errors. Does the
  objective still work when we have statistical errors on this term?
  Moreover, can we reduce the optimization to a sequence of linear
  opt/policy opt-style problems via frank-wolfe?
\item What realizability assumptions do we actually need in order for
  \cref{eq:self_training} to work out?
\end{itemize}

\subsection{To-do list}
To-do list:
\begin{itemize}
\item Exponential weights-type analysis:
  \begin{itemize}
  \item Re-do exp-weights analysis on the objective, but:
    \begin{itemize}
    \item Do the contextual case and handle noise coming from
      contexts. Try to do hellinger stuff and get the fast rate? 
    \item Handle the case where we solve the Exp Weights objective
      using ERM (do the case above first).
    \end{itemize}
    \item Applying the result to entropy:
      \begin{itemize}
      \item Try to get a fast rate for minimizing entropy from
        this guarantee.
      \item Try to directly get a descent lemma for minimizing entropy
        (don't need to go through the full OL analysis).
        \begin{itemize}
        \item This gives parameter convergence, which may be useful
          for other things we want to do.
        \end{itemize}
    \end{itemize}
  \end{itemize}
\item Showing convergence to argmax:
  \begin{itemize}
  \item Focus on the case where $\Pi$ is parameterized via softmax
    policies to start.
  \item Is there a way to directly derive this from the exp
    
    weights/regret-type reasoning above?
  \item Possible idea: Try to argue that we're converging to argmax
    policy in KL/TV distance (via descent lemma-type reasoning?)
  \item Focus on CB case with general $f$ class---vanilla bandit is
    too easy, but this is rich enough to be interesting.
    \begin{itemize}
    \item Idea: Argue that if $\fstar$ has a uniform margin,
      $\fhat\ind{1}$ also has a uniform margin on most $x$s. Then try
      to directly use properties of exp weights updates to argue that
      each update preserves the margin on most $x$s?
    \end{itemize}
  \item Seems like optimization view might be useful here: Argue that
    argmax policy is a local optima, then argue that we converge to it
    in TV? 
  \item Perhaps the easiest way to import the optimization view is to
    handle the CB case, but where $\Pi$ is all possible
    policies. Still have stat errors, but not entirely non-trivial? Or
    maybe it is since we are basically doing the bandit case pointwise
    when $n\gg\log\abs{\Pi}$.
  \end{itemize}
\end{itemize}

\section{Finite-Sample Analysis}

\dfc{Plan for basic analysis
  \begin{itemize}
  \item Work out the best version of gradient dominance condition.
  \item Assume above holds uniformly for all $x$ at initialization.
  \item Argue that this is preserved whp at each iteration.
  \item Do the exp weights analysis, accounting for statistical error
    at each update step and generalization analysis (possibly unboundedness?).
  \end{itemize}
  Open problems for better analysis:
  \begin{itemize}
  \item Strong convexity-type approach? seems like linear convergence
    must be possible.
  \end{itemize}
  }

\subsection{Gradient Dominance}

\subsection{Exponential Weights}



\section{IGW Question}
Consider the contextual bandits
Suppose we solve the following optimization problem:
\[
  \pihat=\argmax_{\pi\in\Pi}\crl*{
    \Eh\brk*{r} - \beta\Dkl{\piref}{\pi}}.
\]
For a fixed choice of $\piref$, this is equivalent to generalized IGW
(``SmoothIGW''), and leads to guarantees that scale with
$\nrm*{\frac{\pistar}{\piref}}$. Suppose instead we do the following
process iteratively:
\[
  \pi\ind{t+1}=\argmax_{\pi\in\Pi}\crl*{
    \Eh\brk*{r} - \beta\Dkl{\pi\ind{t}}{\pi}}.
\]
Can we show that this leads to guarantees that scale with the
coverability parameter for the class $\Pi$?


\bibliography{refs} 

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
