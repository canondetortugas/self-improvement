% \documentclass[11pt]{article}
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

 
% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks 
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{tocloft}            % TOC spacing

%\usepackage{txfont}

\usepackage{enumitem}

\usepackage{breakcites}

\usepackage{etoolbox}
\usepackage{comment}
\newtoggle{draft}
\togglefalse{draft}
\newcommand{\draft}[1]{\iftoggle{draft}{#1}{}}


% \mathscr
\usepackage{mathrsfs}

\usepackage{algorithm}
\usepackage{verbatim}
\usepackage[noend]{algpseudocode}
\newcommand{\multiline}[1]{\parbox[t]{\dimexpr\linewidth-\algorithmicindent}{#1}}

\usepackage{multicol}

\usepackage{colortbl}

\usepackage{setspace}

\usepackage{transparent}

\usepackage{inconsolata}
\usepackage[scaled=.90]{helvet}
%\usepackage{fontspec}
% \usepackage{helvet}
\usepackage{xspace}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{bm}
\newcommand{\x}{\bm{x}}

% \usepackage{eufrak}
% \usepackage{fontspec,unicode-math}
% \setmathfont[range=\mathfrak]{Old English Text MT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Better looking lowercase mathfrak
% https://tex.stackexchange.com/questions/398710/alternative-separate-font-to-mathfrak-for-small-letters

\DeclareFontFamily{U}{jkpmia}{}
\DeclareFontShape{U}{jkpmia}{m}{it}{<->s*jkpmia}{}
\DeclareFontShape{U}{jkpmia}{bx}{it}{<->s*jkpbmia}{}
\DeclareMathAlphabet{\mathfrak}{U}{jkpmia}{m}{it}
\SetMathAlphabet{\mathfrak}{bold}{U}{jkpmia}{bx}{it}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{arxiv_style}
\input{dylan}
\input{macros}
\let\underbar\undefined
\input{widebar}

% \let\openbox\relax
% \usepackage{pxfonts}
% \let\openbox\relax
% \usepackage{txfonts}


\usepackage{color-edits}
 % \usepackage[suppress]{color-edits}
 \addauthor{df}{ForestGreen}
 \newcommand{\dfc}[1]{\dfcomment{#1}}

 % \usepackage{showlabels}

 % \usepackage[cut]{thmbox}

% \input{theorembox}
 


  % Indent option for \Statex
\makeatletter
\let\OldStatex\Statex
\renewcommand{\Statex}[1][3]{%
  \setlength\@tempdima{\algorithmicindent}%
  \OldStatex\hskip\dimexpr#1\@tempdima\relax}
\makeatother

% \let\vec\undefined
 \usepackage{accents}
 \newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\let\oldparagraph\paragraph
\newcommand{\paragraphi}[1]{\oldparagraph{\emph{#1}.}}
\renewcommand{\paragraph}[1]{\oldparagraph{#1.}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \newcommand{\Unif}{\mathsf{Unif}}

\newcommand{\phistar}{\phi^{\star}}
\newcommand{\gstar}{g^{\star}}

\newcommand{\meet}{\wedge} 
\newcommand{\nn}{\nonumber}

\renewcommand{\dagger}{\texttt{DAGGER}\xspace}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\title{}
\title{Self-Improvement and Bootstrapping}
\author{%
%
}


\date{}

\begin{document}
\maketitle

\tableofcontents

\section{Basic self-improvement setup}
Consider the following setup. The true model is $\pistar:\cX\to\Delta(\cY)$. We
initially receive $n$ samples $\cD=\crl*{(x,y)}$ where $x\sim\rho$ and
$y\sim\pistar$, and we fit $\pi\ind{1}
=\argmax_{\pi\in\Pi}\Eh\brk*{\log\pi(y\mid{}x)}$. We then repeatedly
do the following:
\begin{itemize}
\item Generate a dataset $\cD\ind{t}=\crl*{(x,y)}$ of samples with
  $x\sim\rho$ and $y\sim\pi\ind{t}$.
\item Define
  \begin{align}
    \label{eq:self_training}
    \pi\ind{t+1}=\argmax\crl*{
      \Eh_{\cD\ind{t}}\brk*{\log\pi(y\mid{}x)}
      -\beta\Dkl{\pi}{\pi\ind{t}}
      }.
  \end{align}
\end{itemize}
\paragraph{Questions and comments}
We know that with no statistical or optimization errors, this
  should converge to
  \[
    \pi\ind{t+1}(y\mid{}x)\propto{}\pistar(y\mid{}x)^{1+\beta{}T}
  \]
  \dfc{Check this!}
  Questions:
\begin{itemize}
\item When can we achieve similar convergence in the presence of
  statistical errors?
\item Is this purely a computational phenomenon?
  \begin{itemize}
  \item Concretely: Is $\pi\ind{t+1}$ ever better statistically than
    computing the sequence-level argmax
    $\argmax_{y}\pi\ind{1}(y\mid{}x)$? If not, then we want to argue
    that the benefit is that we are competing with the sequence level
    argmax without having to actually compute it (that is, given a new
    $x$, we just need to sample $y\sim{}\pi\ind{t+1}(y\mid{}x)$.
    \begin{itemize}
    \item This would be a sort of ``optimization vs sampling'' phenomenon.
    \end{itemize}
  \item Alternatively, we might be able to argue that our algo *does*
    have statistical benefits, because it allows us to leverage the
    unlabeled examples $x$s.
  \end{itemize}
\item Presumably we need some kind of margin condition to ensure that
  \cref{eq:self_training} actually converges to argmax instead of
  noise. This is most easy to think about when $\pistar$ is itself a
  softmax policy.
\item Suppose $\pi\ind{1}$ is an LLM with temperature parameter
  $\eta$. We have $y=y_{1:H}$, and
  $\pi(y_h\mid{}x,y_{1:h-1})\propto\exp\prn*{\frac{f(x,y_{1:h})}{\eta}}$. A
  good baseline for us is to set $\eta\to{}0$ after training. In this
  case, sampling autoregressively is the same as choosing the greedy
  argmax at each step $h$,
  i.e. $y_h=\argmax{}\pi(y_h\mid{}x,y_{1:h-1})$, but is not
  necessarily equivalent to the sequence-level argmax.
  \begin{itemize}
  \item Hence, we might hope that self-training can improve over this
    baseline, even if it can't improve over the true sequence-level
    argmax.
  \item Good example to think about above: $f=\Qstarb$ is the soft
    Q-function for a given reward function $r$.
  \end{itemize}
\item In reality, we will not be able to evaluate the KL term in
  \cref{eq:self_training} exactly, due to statistical errors. Does the
  objective still work when we have statistical errors on this term?
  Moreover, can we reduce the optimization to a sequence of linear
  opt/policy opt-style problems via frank-wolfe?
\item What realizability assumptions do we actually need in order for
  \cref{eq:self_training} to work out?
\end{itemize}

\subsection{Special case: Multi-Armed Bandits}

\section{IGW Question}
Consider the contextual bandits
Suppose we solve the following optimization problem:
\[
  \pihat=\argmax_{\pi\in\Pi}\crl*{
    \Eh\brk*{r} - \beta\Dkl{\piref}{\pi}}.
\]
For a fixed choice of $\piref$, this is equivalent to generalized IGW
(``SmoothIGW''), and leads to guarantees that scale with
$\nrm*{\frac{\pistar}{\piref}}$. Suppose instead we do the following
process iteratively:
\[
  \pi\ind{t+1}=\argmax_{\pi\in\Pi}\crl*{
    \Eh\brk*{r} - \beta\Dkl{\pi\ind{t}}{\pi}}.
\]
Can we show that this leads to guarantees that scale with the
coverability parameter for the class $\Pi$?


\bibliography{refs} 

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
